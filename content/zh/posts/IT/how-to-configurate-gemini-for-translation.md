---
title: "如何配置AI模型做文本翻译？"
date: 2025-02-26
description:
categories:
  - IT
tags:
  - 互联网
  - 数学&计算机
  - AI
---

我们将使用AI模型做多语种的翻译，以 [Google Vertex AI - Translation - Gemini 1.5 pro](https://console.cloud.google.com/vertex-ai/studio/translation) 为例。

需要配置几个关键参数。


# Output token limit 
输出token令牌限制决定了单次提示的最大文本输出量。一个token大约相当于四个字符。 目前模型的api调取费用非常低，百万token才不到1美元，若中文和各种字符与token兑换比例为1:1，把输入和输出都计费，百万字的文章翻译也不到10美元，普通人可能一年也用不完百万token，超级划算。

参考前文：[AI模型Token和各种字符的换算计量方法](AI-token-count-with-characters.md)

当然这是以开发者的身份的成本计算，如果普通人不会配置云计算平台并调取api，选择使用前端图形化的对话窗口，每个月大约10-20美元的基本订阅费用，普通用户的ai使用成本比开发者高了数十乃至数千倍。

所以这里，我们把参数设置到最大，允许输入输出最长的文章。

如果每天调取api的量很大，需要节约成本，那么需要精细估算，平均文章的字符数，配置参数相对合适，能节约成本。因为参数配置越大，某些模型的成本越高，即使实际输入输出量没有达到最大token限制。



# Temperature 

temperature 的工作原理：

temperature 参数通过调整模型预测的概率分布来影响词汇的选择。具体来说，它会影响每个词汇的概率值。

较高的 temperature 值（例如 0.7-1.0）： 会使概率分布更加平坦，这意味着所有可能的词汇的概率都更加接近。这会增加模型选择不太可能出现的词汇的可能性，从而生成更随机、更具创造性的文本。 缺点是，也更容易出现不准确或不合适的词汇。

较低的 temperature 值（例如 0.1-0.4）： 会使概率分布更加陡峭，这意味着概率最高的词汇的概率会更加突出。这会降低模型选择不太可能出现的词汇的可能性，从而生成更保守、更可预测的文本。 优点是，翻译结果更准确，更不容易出现错误。

temperature = 0.0： 模型将始终选择概率最高的词汇。这会生成非常确定性和可预测的文本，但缺乏创造性。 通常不建议设置为 0，因为可能会导致模型陷入重复循环。


# top-p 

top-p值改变了模型选择输出词的方式。模型会按照概率从高到低选择词，直到词概率之和等于 top-p 值。例如，如果词 A、B 和 C 的概率分别为 0.3、0.2 和 0.1，并且 top-p 值为 0.5，则模型将使用温度参数在 A 或 B 中选择一个作为下一个词。

Top-p 的工作原理：

- 概率排序： 模型会预测所有可能的下一个词（token）并为其分配一个概率。
- 概率累加： 模型按照概率从高到低的顺序累加这些概率值。
- 截止点： 当累加的概率值达到或超过你设置的 top-p 值时，累加停止。
- 词汇选择： 模型从已累加的词汇中随机选择一个作为下一个生成的词。选择的概率与该词汇的原始概率成正比。

top-p 参数的作用：

- 降低 top-p 值（例如 0.1-0.4）： 模型将更加专注于概率最高的几个词，生成的结果更保守、更可预测，也更不容易出现偏离主题或语法错误的词汇。适合对翻译的**准确性要求极高的场景，例如专业术语翻译**。
  
- 提高 top-p 值（例如 0.7-0.95）： 模型会考虑更多可能的词，生成的结果更具多样性、创造性，但也更容易出现一些不准确或不合适的词汇。适合需要一定**创造性的场景，例如文学作品翻译**，但需要注意后期校对。


# 结合使用 temperature 和 top-p

temperature 和 top-p 的区别：

- temperature 调整概率分布： temperature 影响的是所有词汇的概率，它会全局性地调整概率分布的形状。

- top-p 过滤词汇： top-p 则是通过累加概率并设置截止点来过滤掉一部分词汇，模型只会在过滤后的词汇中进行选择。

temperature 和 top-p 的功能类似，都是用来控制模型生成文本时的随机性和创造性，但它们的工作方式略有不同。 理解它们的差异以及如何结合使用，对于获得最佳的翻译结果至关重要。

temperature 和 top-p 可以结合使用，以实现更精细的控制。

- 保守且准确的翻译： 如果你希望获得非常准确且保守的翻译结果，可以同时降低 temperature 和 top-p 的值。 例如，temperature = 0.2 和 top-p = 0.3。

- 创造性且多样化的翻译： 如果你希望获得更具创造性和多样化的翻译结果，可以同时提高 temperature 和 top-p 的值。 例如，temperature = 0.7 和 top-p = 0.9。

先用 top-p 过滤，再用 temperature 调整： 一种常见的策略是先使用 top-p 过滤掉不太可能出现的词汇，然后再使用 temperature 在剩余的词汇中调整概率分布。这样可以确保模型只考虑相对合理的词汇，同时又能保持一定的随机性。

#

鉴于您的目标是在准确性和自然度之间取得平衡，并且您使用的是相对较低的温度（0.2-0.4），以下是 Top-P 值的细分和建议：

**了解 Top-P 和温度的交互作用**

* **温度：** 控制模型的随机性。较低的温度（如 0.2-0.4）使模型更有可能选择高概率的词，从而产生更可预测和“真实”的输出。较高的温度会增加选择不太可能的词的机会，使输出更具创造性、多样性，并可能更容易出现“幻觉”（编造内容）。

* **Top-P（核采样）：** 控制所考虑词的*多样性*。Top-P 不是考虑*所有*可能的下一个词（数量可能非常庞大），而是专注于一个子集。它的工作原理是：
   1. 按概率对所有可能的下一个词进行排序（从高到低）。
   2. 从最有可能的词开始，将这些词的概率加起来。
   3. 当累积概率达到或超过 Top-P 值时停止。
   4. 然后，模型*仅*从这个选定的词集中随机采样。

**推荐的 Top-P 值（温度较低，为 0.2-0.4）**

* **0.8 - 0.95：** 这是 Top-P 的普遍推荐范围，适用于希望在多样性和连贯性之间取得良好平衡的情况。在低温下，此范围内的 Top-P 仍然允许词语选择和句子结构存在一些变化，使翻译听起来更自然，而低温则使其忠实于原文。

* **0.95 - 0.99：** 如果您发现 0.8-0.95 的输出*过于*确定性（例如，对于相似的句子总是使用相同的措辞），则可以稍微增加 Top-P。这将在采样池中包含更多不太可能的词，从而增加多样性。但是，请谨慎操作：在低温下，Top-P 过高可能会开始引入轻微的不准确或笨拙的措辞，因为模型被迫从不太可能的选项中进行选择。

* **1.0：** Top-P 为 1.0 实际上*禁用*了 Top-P 采样。模型会考虑*所有*可能的词。在低温下，这通常会导致模型始终选择最可能的词，从而使输出非常确定性，并可能出现重复。我通常*不*建议将 Top-P 设置为 1.0，除非您特别希望获得最“可预测”的输出。

* **低于 0.8：** 通常*不*建议使用远低于 0.8 的值，尤其是在低温下。这将严重限制模型的选择，可能导致不自然甚至荒谬的翻译。模型可能会“卡住”，重复相同的词语或短语。

**为什么 0.9 是一个好的起点：**

* **平衡多样性和连贯性：** 0.9 允许在不冒险进入低概率领域的情况下提供合理的词语选择范围。
* **补充低温：** 低温（0.2-0.4）已经鼓励模型倾向于选择高概率的词。Top-P 为 0.9 可以轻微地促进多样性，而不会牺牲准确性。
* **减少重复：** 与 Top-P = 1.0 相比，0.9 的值可以显著降低模型陷入重复模式的可能性。
* **良好的默认值：** 在许多文本生成任务中，0.9 通常是 Top-P 的良好默认值。

**建议和迭代：**

1. **从 Top-P = 0.9 开始。** 这是一个可靠的起点。
2. **评估：** 仔细检查翻译后的输出。寻找：
   * **准确性：** 翻译是否忠实于原文？
   * **自然度：** 翻译是否流畅，听起来像人写的？
   * **重复：** 是否有任何重复的短语或词语选择？
3. **调整：**
   * 如果输出过于确定性或重复，请稍微*增加* Top-P（例如，增加到 0.95）。
   * 如果输出包含不准确或笨拙的措辞，请稍微*减少* Top-P（例如，减少到 0.85 或 0.8）。
   * 如果您仍然不满意，可以考虑稍微提高温度（例如，提高到 0.5），但要谨慎操作，因为这会增加出现幻觉的风险。

重要的是进行实验，找到最适合您的特定文档和所需翻译风格的设置。最佳方法是迭代：翻译文件的样本，评估结果，调整参数，然后重复操作。 